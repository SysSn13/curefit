name: Deprecated - use crawl_data.yml

on:
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Run crawler
        env:
          # Optional: provide CULTFIT_COOKIE_STRING if you want to fetch premium packs
          CULTFIT_COOKIE_STRING: ${{ secrets.CULTFIT_COOKIE_STRING }}
        run: |
          python cultfit_crawler.py

      - name: Install frontend deps & build
        working-directory: frontend
        run: |
          npm ci --ignore-scripts
          npm run build

      - name: Copy build + data to docs
        run: |
          rm -rf docs
          mkdir -p docs
          cp -R frontend/dist/* docs/
          cp data/media_by_section.json docs/

      - name: Commit README updates
        uses: EndBug/add-and-commit@v9
        with:
          add: |
            README.md
            docs/
          message: 'chore(readme): automatic weekly refresh'
          default_author: github_actions 